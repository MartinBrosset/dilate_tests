{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import pandas as pd\n",
    "from tslearn.metrics import dtw, dtw_path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mod√®le Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, fc_units, output_size, target_length, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.target_length = target_length\n",
    "        self.device = device\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_gru = nn.GRU(input_size=output_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.decoder_fc = nn.Linear(hidden_size, fc_units)\n",
    "        self.decoder_out = nn.Linear(fc_units, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
    "        _, encoder_hidden = self.encoder_gru(x, encoder_hidden)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_input = x[:, -1, :].unsqueeze(1)  # Last element of input sequence\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        outputs = torch.zeros(batch_size, self.target_length, x.size(2), device=self.device)\n",
    "        for ti in range(self.target_length):\n",
    "            decoder_output, decoder_hidden = self.decoder_gru(decoder_input, decoder_hidden)\n",
    "            decoder_output = F.relu(self.decoder_fc(decoder_output))\n",
    "            decoder_output = self.decoder_out(decoder_output)\n",
    "            outputs[:, ti:ti+1, :] = decoder_output\n",
    "            decoder_input = decoder_output  # Use own predictions as inputs for next step\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(x, y=None):\n",
    "    '''\n",
    "    Input: x is a Nxd matrix\n",
    "           y is an optional Mxd matirx\n",
    "    Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
    "            if y is not given then use 'y=x'.\n",
    "    i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
    "    '''\n",
    "    x_norm = (x**2).sum(1).view(-1, 1)\n",
    "    if y is not None:\n",
    "        y_t = torch.transpose(y, 0, 1)\n",
    "        y_norm = (y**2).sum(1).view(1, -1)\n",
    "    else:\n",
    "        y_t = torch.transpose(x, 0, 1)\n",
    "        y_norm = x_norm.view(1, -1)\n",
    "    \n",
    "    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
    "    return torch.clamp(dist, 0.0, float('inf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython = True)\n",
    "def compute_softdtw(D, gamma):\n",
    "  N = D.shape[0]\n",
    "  M = D.shape[1]\n",
    "  R = np.zeros((N + 2, M + 2)) + 1e8\n",
    "  R[0, 0] = 0\n",
    "  for j in range(1, M + 1):\n",
    "    for i in range(1, N + 1):\n",
    "      ### softmin\n",
    "      r0 = -R[i - 1, j - 1] / gamma\n",
    "      r1 = -R[i - 1, j] / gamma\n",
    "      r2 = -R[i, j - 1] / gamma\n",
    "      rmax = max(max(r0, r1), r2)\n",
    "      rsum = np.exp(r0 - rmax) + np.exp(r1 - rmax) + np.exp(r2 - rmax)\n",
    "      softmin = - gamma * (np.log(rsum) + rmax)\n",
    "      R[i, j] = D[i - 1, j - 1] + softmin\n",
    "  return R\n",
    "\n",
    "@jit(nopython = True)\n",
    "def compute_softdtw_backward(D_, R, gamma):\n",
    "  N = D_.shape[0]\n",
    "  M = D_.shape[1]\n",
    "  D = np.zeros((N + 2, M + 2))\n",
    "  E = np.zeros((N + 2, M + 2))\n",
    "  D[1:N + 1, 1:M + 1] = D_\n",
    "  E[-1, -1] = 1\n",
    "  R[:, -1] = -1e8\n",
    "  R[-1, :] = -1e8\n",
    "  R[-1, -1] = R[-2, -2]\n",
    "  for j in range(M, 0, -1):\n",
    "    for i in range(N, 0, -1):\n",
    "      a0 = (R[i + 1, j] - R[i, j] - D[i + 1, j]) / gamma\n",
    "      b0 = (R[i, j + 1] - R[i, j] - D[i, j + 1]) / gamma\n",
    "      c0 = (R[i + 1, j + 1] - R[i, j] - D[i + 1, j + 1]) / gamma\n",
    "      a = np.exp(a0)\n",
    "      b = np.exp(b0)\n",
    "      c = np.exp(c0)\n",
    "      E[i, j] = E[i + 1, j] * a + E[i, j + 1] * b + E[i + 1, j + 1] * c\n",
    "  return E[1:N + 1, 1:M + 1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class SoftDTWBatch(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, D, gamma = 1.0): # D.shape: [batch_size, N , N]\n",
    "        dev = D.device\n",
    "        batch_size,N,N = D.shape\n",
    "        gamma = torch.FloatTensor([gamma]).to(dev)\n",
    "        D_ = D.detach().cpu().numpy()\n",
    "        g_ = gamma.item()\n",
    "\n",
    "        total_loss = 0\n",
    "        R = torch.zeros((batch_size, N+2 ,N+2)).to(dev)   \n",
    "        for k in range(0, batch_size): # loop over all D in the batch    \n",
    "            Rk = torch.FloatTensor(compute_softdtw(D_[k,:,:], g_)).to(dev)\n",
    "            R[k:k+1,:,:] = Rk\n",
    "            total_loss = total_loss + Rk[-2,-2]\n",
    "        ctx.save_for_backward(D, R, gamma)\n",
    "        return total_loss / batch_size\n",
    "  \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        dev = grad_output.device\n",
    "        D, R, gamma = ctx.saved_tensors\n",
    "        batch_size,N,N = D.shape\n",
    "        D_ = D.detach().cpu().numpy()\n",
    "        R_ = R.detach().cpu().numpy()\n",
    "        g_ = gamma.item()\n",
    "\n",
    "        E = torch.zeros((batch_size, N ,N)).to(dev) \n",
    "        for k in range(batch_size):         \n",
    "            Ek = torch.FloatTensor(compute_softdtw_backward(D_[k,:,:], R_[k,:,:], g_)).to(dev)\n",
    "            E[k:k+1,:,:] = Ek\n",
    "\n",
    "        return grad_output * E, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Soft DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jit(nopython = True)\n",
    "def my_max(x, gamma):\n",
    "    # use the log-sum-exp trick\n",
    "    max_x = np.max(x)\n",
    "    exp_x = np.exp((x - max_x) / gamma)\n",
    "    Z = np.sum(exp_x)\n",
    "    return gamma * np.log(Z) + max_x, exp_x / Z\n",
    "\n",
    "@jit(nopython = True)\n",
    "def my_min(x,gamma) :\n",
    "    min_x, argmax_x = my_max(-x, gamma)\n",
    "    return - min_x, argmax_x\n",
    "\n",
    "@jit(nopython = True)\n",
    "def my_min_hessian_product(p, z, gamma):\n",
    "    return - ( p * z - p * np.sum(p * z) ) /gamma\n",
    "\n",
    "@jit(nopython = True)\n",
    "def dtw_grad(theta, gamma):\n",
    "    m = theta.shape[0]\n",
    "    n = theta.shape[1]\n",
    "    V = np.zeros((m + 1, n + 1))\n",
    "    V[:, 0] = 1e10\n",
    "    V[0, :] = 1e10\n",
    "    V[0, 0] = 0\n",
    "\n",
    "    Q = np.zeros((m + 2, n + 2, 3))\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            # theta is indexed starting from 0.\n",
    "            v, Q[i, j] = my_min(np.array([V[i, j - 1],\n",
    "                                                V[i - 1, j - 1],\n",
    "                                                V[i - 1, j]]) , gamma)                        \n",
    "            V[i, j] = theta[i - 1, j - 1] + v\n",
    "\n",
    "    E = np.zeros((m + 2, n + 2))\n",
    "    E[m + 1, :] = 0\n",
    "    E[:, n + 1] = 0\n",
    "    E[m + 1, n + 1] = 1\n",
    "    Q[m + 1, n + 1] = 1\n",
    "\n",
    "    for i in range(m,0,-1):\n",
    "        for j in range(n,0,-1):\n",
    "            E[i, j] = Q[i, j + 1, 0] * E[i, j + 1] + \\\n",
    "                      Q[i + 1, j + 1, 1] * E[i + 1, j + 1] + \\\n",
    "                      Q[i + 1, j, 2] * E[i + 1, j]\n",
    "    \n",
    "    return V[m, n], E[1:m + 1, 1:n + 1], Q, E\n",
    "\n",
    "\n",
    "@jit(nopython = True)\n",
    "def dtw_hessian_prod(theta, Z, Q, E, gamma):\n",
    "    m = Z.shape[0]\n",
    "    n = Z.shape[1]\n",
    "\n",
    "    V_dot = np.zeros((m + 1, n + 1))\n",
    "    V_dot[0, 0] = 0\n",
    "\n",
    "    Q_dot = np.zeros((m + 2, n + 2, 3))\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            # theta is indexed starting from 0.\n",
    "            V_dot[i, j] = Z[i - 1, j - 1] + \\\n",
    "                          Q[i, j, 0] * V_dot[i, j - 1] + \\\n",
    "                          Q[i, j, 1] * V_dot[i - 1, j - 1] + \\\n",
    "                          Q[i, j, 2] * V_dot[i - 1, j]\n",
    "\n",
    "            v = np.array([V_dot[i, j - 1], V_dot[i - 1, j - 1], V_dot[i - 1, j]])\n",
    "            Q_dot[i, j] = my_min_hessian_product(Q[i, j], v, gamma)\n",
    "    E_dot = np.zeros((m + 2, n + 2))\n",
    "\n",
    "    for j in range(n,0,-1):\n",
    "        for i in range(m,0,-1):\n",
    "            E_dot[i, j] = Q_dot[i, j + 1, 0] * E[i, j + 1] + \\\n",
    "                          Q[i, j + 1, 0] * E_dot[i, j + 1] + \\\n",
    "                          Q_dot[i + 1, j + 1, 1] * E[i + 1, j + 1] + \\\n",
    "                          Q[i + 1, j + 1, 1] * E_dot[i + 1, j + 1] + \\\n",
    "                          Q_dot[i + 1, j, 2] * E[i + 1, j] + \\\n",
    "                          Q[i + 1, j, 2] * E_dot[i + 1, j]\n",
    "\n",
    "    return V_dot[m, n], E_dot[1:m + 1, 1:n + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "class PathDTWBatch(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, D, gamma): # D.shape: [batch_size, N , N]\n",
    "        batch_size,N,N = D.shape\n",
    "        device = D.device\n",
    "        D_cpu = D.detach().cpu().numpy()\n",
    "        gamma_gpu = torch.FloatTensor([gamma]).to(device)\n",
    "        \n",
    "        grad_gpu = torch.zeros((batch_size, N ,N)).to(device)\n",
    "        Q_gpu = torch.zeros((batch_size, N+2 ,N+2,3)).to(device)\n",
    "        E_gpu = torch.zeros((batch_size, N+2 ,N+2)).to(device)  \n",
    "        \n",
    "        for k in range(0,batch_size): # loop over all D in the batch    \n",
    "            _, grad_cpu_k, Q_cpu_k, E_cpu_k = dtw_grad(D_cpu[k,:,:], gamma)     \n",
    "            grad_gpu[k,:,:] = torch.FloatTensor(grad_cpu_k).to(device)\n",
    "            Q_gpu[k,:,:,:] = torch.FloatTensor(Q_cpu_k).to(device)\n",
    "            E_gpu[k,:,:] = torch.FloatTensor(E_cpu_k).to(device)\n",
    "        ctx.save_for_backward(grad_gpu,D, Q_gpu ,E_gpu, gamma_gpu) \n",
    "        return torch.mean(grad_gpu, dim=0) \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        device = grad_output.device\n",
    "        grad_gpu, D_gpu, Q_gpu, E_gpu, gamma = ctx.saved_tensors\n",
    "        D_cpu = D_gpu.detach().cpu().numpy()\n",
    "        Q_cpu = Q_gpu.detach().cpu().numpy()\n",
    "        E_cpu = E_gpu.detach().cpu().numpy()\n",
    "        gamma = gamma.detach().cpu().numpy()[0]\n",
    "        Z = grad_output.detach().cpu().numpy()\n",
    "        \n",
    "        batch_size,N,N = D_cpu.shape\n",
    "        Hessian = torch.zeros((batch_size, N ,N)).to(device)\n",
    "        for k in range(0,batch_size):\n",
    "            _, hess_k = dtw_hessian_prod(D_cpu[k,:,:], Z, Q_cpu[k,:,:,:], E_cpu[k,:,:], gamma)\n",
    "            Hessian[k:k+1,:,:] = torch.FloatTensor(hess_k).to(device)\n",
    "\n",
    "        return  Hessian, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss DILATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate_loss(outputs, targets, alpha, gamma, device):\n",
    "\t# outputs, targets: shape (batch_size, N_output, 1)\n",
    "\tbatch_size, N_output = outputs.shape[0:2]\n",
    "\tloss_shape = 0\n",
    "\tsoftdtw_batch = SoftDTWBatch.apply\n",
    "\tD = torch.zeros((batch_size, N_output,N_output )).to(device)\n",
    "\tfor k in range(batch_size):\n",
    "\t\tDk = pairwise_distances(targets[k,:,:].view(-1,1),outputs[k,:,:].view(-1,1))\n",
    "\t\tD[k:k+1,:,:] = Dk     \n",
    "\tloss_shape = softdtw_batch(D,gamma)\n",
    "\t\n",
    "\tpath_dtw = PathDTWBatch.apply\n",
    "\tpath = path_dtw(D,gamma)           \n",
    "\tOmega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n",
    "\tloss_temporal =  torch.sum( path*Omega ) / (N_output*N_output) \n",
    "\tloss = alpha*loss_shape+ (1-alpha)*loss_temporal\n",
    "\treturn loss, loss_shape, loss_temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entra√Ænement et pr√©diction sur le dataset ECG5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEED POUR LA REPRODUCTIBILITE \n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# parameters\n",
    "batch_size = 35\n",
    "N = 500\n",
    "N_input = 84\n",
    "N_output = 56  \n",
    "sigma = 0.01\n",
    "gamma = 0.01\n",
    "\n",
    "### IMPORTATION DU DATASET ECG\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "\n",
    "ecg_train = np.array(pd.read_table(DATA_PATH + \"ECG5000/ECG5000_TRAIN.tsv\"))[:, :, np.newaxis]\n",
    "ecg_test = np.array(pd.read_table(DATA_PATH + \"ECG5000/ECG5000_TEST.tsv\"))[:, :, np.newaxis]\n",
    "\n",
    "\n",
    "# Normalisation\n",
    "ecg_train_flat = ecg_train.reshape(-1, ecg_train.shape[1])\n",
    "ecg_test_flat = ecg_test.reshape(-1, ecg_test.shape[1])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "ecg_train_flat = scaler.fit_transform(ecg_train_flat)\n",
    "ecg_test_flat = scaler.transform(ecg_test_flat)\n",
    "\n",
    "ecg_train = ecg_train_flat.reshape(ecg_train.shape[0], ecg_train.shape[1], 1)\n",
    "ecg_test = ecg_test_flat.reshape(ecg_test.shape[0], ecg_test.shape[1], 1)\n",
    "\n",
    "\n",
    "# On tronque pour s'assurer que la taille est un multiple de batch_size\n",
    "num_train_batches = ecg_train.shape[0] // batch_size\n",
    "ecg_train = ecg_train[:num_train_batches * batch_size]\n",
    "num_test_batches = ecg_test.shape[0] // batch_size\n",
    "ecg_test = ecg_test[:num_test_batches * batch_size]\n",
    "\n",
    "print(ecg_train.shape, ecg_test.shape)\n",
    "\n",
    "\n",
    "class ECG5000Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, output_length=56):\n",
    "        self.data = torch.from_numpy(data).to(dtype=torch.float32)\n",
    "        self.output_length = output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index, :-self.output_length], self.data[index, -self.output_length:]\n",
    "    \n",
    "ecg_train_dataset = ECG5000Dataset(ecg_train)\n",
    "ecg_test_dataset = ECG5000Dataset(ecg_test)\n",
    "trainloader = DataLoader(ecg_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(ecg_test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions d'entra√Ænement et d'√©valuation des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net,loss_type, learning_rate, epochs=1000, gamma = 0.001,\n",
    "                print_every=50, alpha=0.5):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
    "    ### learning rate adaptatif qui diminue au cours des epochs\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, target = data\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "            target = torch.tensor(target, dtype=torch.float32).to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_mse,loss_shape,loss_temporal = torch.tensor(0),torch.tensor(0),torch.tensor(0)\n",
    "            \n",
    "            if (loss_type=='mse'):\n",
    "                loss_mse = criterion(target,outputs)\n",
    "                loss = loss_mse                   \n",
    " \n",
    "            if (loss_type=='dilate'):    \n",
    "                loss, loss_shape, loss_temporal = dilate_loss(target,outputs,alpha, gamma, device)             \n",
    "                  \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\n",
    "        ### on ajoute un pas au scheduler pour mettre le learning rate √† jour\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch % print_every == 0):\n",
    "            print('epoch ', epoch, ' loss ',loss.item(),' loss shape ',loss_shape.item(),' loss temporal ',loss_temporal.item())\n",
    "            m, d, t = eval_model(net,testloader, gamma,verbose=1)\n",
    "\n",
    "  \n",
    "\n",
    "def eval_model(net,loader, gamma,verbose=1):   \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    losses_mse = []\n",
    "    losses_dtw = []\n",
    "    losses_tdi = []   \n",
    "\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        loss_mse, loss_dtw, loss_tdi = torch.tensor(0),torch.tensor(0),torch.tensor(0)\n",
    "        # get the inputs\n",
    "        inputs, target = data\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        target = torch.tensor(target, dtype=torch.float32).to(device)\n",
    "        batch_size, N_output = target.shape[0:2]\n",
    "        outputs = net(inputs)\n",
    "         \n",
    "        # MSE    \n",
    "        loss_mse = criterion(target,outputs)    \n",
    "\n",
    "        # DTW and TDI\n",
    "        loss_dtw, loss_tdi = 0,0\n",
    "        for k in range(batch_size):         \n",
    "            target_k_cpu = target[k,:,0:1].view(-1).detach().cpu().numpy()\n",
    "            output_k_cpu = outputs[k,:,0:1].view(-1).detach().cpu().numpy()\n",
    "\n",
    "            path, sim = dtw_path(target_k_cpu, output_k_cpu)   \n",
    "            loss_dtw += sim\n",
    "                       \n",
    "            Dist = 0\n",
    "            for i,j in path:\n",
    "                    Dist += (i-j)*(i-j)\n",
    "            loss_tdi += Dist / (N_output*N_output)            \n",
    "                        \n",
    "        loss_dtw = loss_dtw /batch_size\n",
    "        loss_tdi = loss_tdi / batch_size\n",
    "\n",
    "        # print statistics\n",
    "        losses_mse.append( loss_mse.item() )\n",
    "        losses_dtw.append( loss_dtw )\n",
    "        losses_tdi.append( loss_tdi )\n",
    "\n",
    "    print( ' Eval mse= ', np.array(losses_mse).mean() ,' dtw= ',np.array(losses_dtw).mean() ,' tdi= ', np.array(losses_tdi).mean()) \n",
    "    return(np.array(losses_mse).mean(), np.array(losses_dtw).mean(), np.array(losses_tdi).mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entra√Ænement des mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Entra√Ænement du mod√®le Seq2Seq avec DILATE')\n",
    "net_gru_dilate = Seq2Seq(input_size=1, hidden_size=128, num_layers=1, fc_units=16, output_size=1, target_length=N_output, device=device).to(device)\n",
    "train_model(net_gru_dilate,loss_type='dilate',learning_rate=0.005, epochs=500, gamma=gamma, alpha=0.5, print_every=20)\n",
    "final_mse, final_dtw, final_tdi = eval_model(net_gru_dilate, testloader, gamma)\n",
    "print()\n",
    "\n",
    "print('Entra√Ænement du mod√®le Seq2Seq avec MSE')\n",
    "net_gru_mse = Seq2Seq(input_size=1, hidden_size=128, num_layers=1, fc_units=16, output_size=1, target_length=N_output, device=device).to(device)\n",
    "train_model(net_gru_mse,loss_type='mse',learning_rate=0.005, epochs=150, gamma=gamma, print_every=15)\n",
    "final_mse_2, final_dtw_2, final_tdi_2 = eval_model(net_gru_mse, testloader, gamma)\n",
    "print()\n",
    "\n",
    "print('Entra√Ænement du mod√®le Seq2Seq avec la soft DTW (dilate avec alpha = 1)')\n",
    "net_gru_soft_dtw = Seq2Seq(input_size=1, hidden_size=128, num_layers=1, fc_units=16, output_size=1, target_length=N_output, device=device).to(device)\n",
    "train_model(net_gru_soft_dtw,loss_type='dilate',learning_rate=0.005, epochs=500, gamma=gamma, alpha =1, print_every=20)\n",
    "final_mse_3, final_dtw_3, final_tdi_3 = eval_model(net_gru_soft_dtw, testloader, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TABLEAU RECAPITULATIF DES METRICS\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Loss' : ['DILATE', 'MSE', 'SOFT DTW'],\n",
    "    'MSE': [final_mse, final_mse_2, final_mse_3],\n",
    "    'DTW': [final_dtw, final_dtw_2, final_dtw_3],\n",
    "    'TDI': [final_tdi, final_tdi_2, final_tdi_3]\n",
    "})\n",
    "\n",
    "print(\"Evaluation des mod√®les : \")\n",
    "display(metrics_df)\n",
    "\n",
    "### PREDICTION DE QUELQUES ECG\n",
    "\n",
    "gen_test = iter(testloader)\n",
    "test_inputs, test_targets = next(gen_test)\n",
    "\n",
    "test_inputs  = torch.tensor(test_inputs, dtype=torch.float32).to(device)\n",
    "test_targets = torch.tensor(test_targets, dtype=torch.float32).to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "nets = [net_gru_mse,net_gru_dilate,net_gru_soft_dtw]\n",
    "\n",
    "for ind in [6,8,12,13,14,15]: ### Quelques exemples parlants\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.rcParams['figure.figsize'] = (17.0,5.0)  \n",
    "    k = 1\n",
    "    for net in nets:\n",
    "        pred = net(test_inputs).to(device)\n",
    "\n",
    "        input = test_inputs.detach().cpu().numpy()[ind,:,:]\n",
    "        target = test_targets.detach().cpu().numpy()[ind,:,:]\n",
    "        preds = pred.detach().cpu().numpy()[ind,:,:]\n",
    "\n",
    "        plt.subplot(1,3,k)\n",
    "        plt.plot(range(0, len(input)), input.flatten(), label='input', linewidth=3)\n",
    "        plt.plot(range(len(input)-1,len(input)+len(preds)), np.concatenate([ input[len(input)-1:len(input)].flatten(), target.flatten() ]) ,label='target',linewidth=3)   \n",
    "        plt.plot(range(len(input)-1,len(input)+len(preds)),  np.concatenate([ input[len(input)-1:len(input)].flatten(), preds.flatten() ])  ,label='prediction',linewidth=3)       \n",
    "        plt.xticks(range(0,140,10))\n",
    "        plt.legend()\n",
    "        k = k+1\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
